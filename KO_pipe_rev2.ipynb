{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Learning Accelerator- Langchain Workflow\n",
    "\n",
    "The purpose of the notebook is to generate useful knowledge objects that will explain data science concepts and provide references and guidance for people who are interested in learning more about each concept. The idea here is that we will leverage langchain and large language models to generate the basic text so that we can take on the role of editor and refine the generated articles to produce high quality and accurate results at a scale that we would not be able to achieve when creating the knowledge objects from scratch on our own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Initial Setup and API Key Management](#1-initial-setup)\n",
    "2. [Few Shot Templates](#2-set-up-fewshot-templates) \n",
    "3. [Setup Large Language Models for Prompt Pipeline](#3-set-up-the-llms)\n",
    "4. [Setup the Functions to Build Knowledge Object Articles](#4-setup-our-knowledge-object-building-function)\n",
    "5. [5. Build a Dataframe and Output Markdown Rough Draft Articles](#5-setup-our-functions-and-dataframe-for-storing-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Initial Setup <a class=\"anchor\" id=\"Initial Setup\"></a>\n",
    "\n",
    "Here we will setup our langchain api keys, and import our list of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows for saving variables to a pickle backup file.\n",
    "#To save use: save(backup_file_name, variable_name1, var_name_2, ...)\n",
    "#To load a backup file use: load_pickle(backup_file_name)\n",
    "\n",
    "import pickle\n",
    "\n",
    "#save pickle files for variables\n",
    "def save(filename, *args):\n",
    "    # Get global dictionary\n",
    "    glob = globals()\n",
    "    d = {}\n",
    "    for v in args:\n",
    "        # Copy over desired values\n",
    "        d[v] = glob[v]\n",
    "    with open(filename, 'wb') as f:\n",
    "        # Put them in the file\n",
    "        pickle.dump(d, f)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    # Get global dictionary\n",
    "    glob = globals()\n",
    "    with open(filename, 'rb') as f:\n",
    "        for k, v in pickle.load(f).items():\n",
    "            # Set each global variable to the value from the file\n",
    "            glob[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To setup a langchain API key see: https://docs.smith.langchain.com/setup\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY']  = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To setup a Google Gemini key see: https://ai.google.dev/tutorials/setup\n",
    "\n",
    "os.environ['GOOGLE_API_KEY']  = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use Hugginface Hub the API token is needed, for info see: https://huggingface.co/docs/hub/security-tokens\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']  = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"]  = getpass()\n",
    "anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ollama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phishing and Social Engineering Detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vulnerability Assessment and Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word Embeddings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Splunk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Subject\n",
       "0                                     Ollama\n",
       "1  Phishing and Social Engineering Detection\n",
       "2    Vulnerability Assessment and Management\n",
       "3                            Word Embeddings\n",
       "4                                     Splunk"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our topics list CSV file\n",
    "import pandas as pd\n",
    "\n",
    "csv_path='topics_502.csv'\n",
    "topic_list = pd.read_csv(csv_path)\n",
    "topic_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Set Up Fewshot Templates\n",
    "\n",
    "In this section we will create example questions and answers to show the LLM models what we would like the results to look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some embeddings, below are all the embeddings that were used for the project, but the one that we used in the final output is left uncommented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pete/Documents/ML_code/mlenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# open ai text embeddings\n",
    "# gpt_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# gpt_embeddings = OpenAIEmbeddings(openai_api_key=gpt_key,model='text-embedding-3-large')\n",
    "\n",
    "\n",
    "# This embedding is using a model from huggingface hub-- this is experimental and may not work as is.\n",
    "# HF_key = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "# huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "#     api_key = HF_key,\n",
    "#     model_name=\"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "# )\n",
    "\n",
    "# This embedding is using BAA model from huggingface (a popular open source model)\n",
    "BAA_model_name = \"BAAI/bge-small-en\"\n",
    "BAA_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "baa = HuggingFaceBgeEmbeddings(\n",
    "    model_name=BAA_model_name, model_kwargs=BAA_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# This is the official google embeddings module using embeddings-001 as the model.\n",
    "# gg_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "# gg_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=gg_key)\n",
    "\n",
    "# # This is the basic Huggingface embedding module:\n",
    "# hf_embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# This is the llamacpp embeddings module-- this is experimental and may not work yet as written.\n",
    "# lcp_embeddings = LlamaCppEmbeddings(model_path=\"/home/pete/models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create our examples with example questions and the resulting answers. This will give guidance to the LLM on what we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {   \"question\": \"\"\"Create an educational article that a data scientist with some college education can use \n",
    "        to teach themselves about Anomaly Detection. Please answer the following questions with each section\n",
    "        of the response:\n",
    "        Explain the core principles or components and brief history of Anomaly Detection..\n",
    "        What are some applications where a data scientist can use Anomaly Detection?\n",
    "        When Should Anomaly Detection be utilized?\n",
    "        What specific technologies should a data scientist focus on in order to become an\n",
    "        expert in Anomaly Detection? Please include specific technologies by name, along with where\n",
    "        a data scientist can learn more about these specific technologies. \n",
    "        authoritative institutions that are important to Anomaly Detection.\n",
    "        What are the strengths of Anomaly Detection?\n",
    "        What are the Limitations of Anomaly Detection?\n",
    "        What are Alternative Options to Anomaly Detection?\n",
    "        List the most common terminology associated with Anomaly Detection and give a brief definition for \n",
    "        each.\n",
    "        What are some Example Deployments of Anomaly Detection?\n",
    "        Please list some beginner level and intermediate level resources for someone learning \n",
    "        about Anomaly Detection to use in order to gain expertise and background knowledge. \n",
    "    \"\"\",\n",
    "        \"answer\": \"\"\"# Anomaly Detection\n",
    "\n",
    "\n",
    "        ## Summary of Anomaly Detection:\n",
    "        \n",
    "        Anomaly detection is a technique used by data scientists to identify unusual or rare events, outliers, or patterns \n",
    "        within a dataset. It involves identifying deviations from normal behavior or expected outcomes, which can be useful \n",
    "        for detecting fraud, diagnosing diseases, monitoring network security, and more. The field has its roots in statistical \n",
    "        methods like control charts, clustering, and outlier detection, with recent advancements driven by machine learning \n",
    "        techniques like one-class SVM, isolation forests, and autoencoders.\n",
    "\n",
    "        ## Applications :\n",
    "        \n",
    "        Anomaly detection has a wide range of applications across various domains, including:\n",
    "        - Finance: detecting fraudulent transactions, identifying insider trading, or predicting stock market crashes.\n",
    "        - Healthcare: diagnosing diseases, monitoring patient health, or detecting medical equipment failures.\n",
    "        - Retail: identifying shoplifting, preventing inventory theft, or detecting supply chain disruptions.\n",
    "        - Cybersecurity: detecting intrusions, identifying malware, or monitoring network traffic for anomalies.\n",
    "        - Manufacturing: detecting equipment failures, monitoring product quality, or predicting maintenance needs.\n",
    "        - Environmental monitoring: detecting pollution, monitoring climate change, or predicting natural disasters.\n",
    "\n",
    "        ## When Should Anomaly Detection be utilized?\n",
    "        \n",
    "        Anomaly detection should be utilized when there's a need to identify rare or unusual events within a dataset, or when \n",
    "        there's a need to monitor system behavior for deviations from expected norms. It's particularly useful when dealing \n",
    "        with high-dimensional data, where traditional statistical methods might not be applicable or efficient.\n",
    "\n",
    "        ## Focus Areas\n",
    "        \n",
    "        A data scientist should focus on learning about machine learning techniques like one-class SVM, isolation forests, autoencoders, \n",
    "        as well as statistical methods like control charts, clustering, and outlier detection. Some specific technologies include:\n",
    "        - One-class SVM (Support Vector Machine): A technique that can detect outliers by constructing a hyperplane that \n",
    "        separates data points from a \"pseudo-outlier\" class.\n",
    "        - Isolation Forests: A method that detects anomalies by growing multiple decision trees on random subsets of data, \n",
    "        and identifying deviant instances based on their unique feature subsets.\n",
    "        - Autoencoders: A neural network architecture that learns data representations by encoding input data into a lower-dimensional \n",
    "        space, and decoding it back to its original form. Anomalies can be detected by comparing the encoded data with its reconstruction.\n",
    "        - Control Charts: A statistical method that monitors the behavior of a process variable over time, by constructing \n",
    "        moving averages and standard deviations, and identifying deviations from expected norms.\n",
    "        - Clustering: A technique that groups data points based on their similarity, by minimizing within-cluster variance or maximizing \n",
    "        between-cluster separation. Outliers can be identified as data points that do not belong to any cluster or are far from any cluster.\n",
    "        - Outlier Detection: A method that identifies outliers by computing summary statistics like mean, median, or standard deviation, \n",
    "        and identifying data points that deviate significantly from these norms.\n",
    "\n",
    "        ## Strengths\n",
    "        \n",
    "        The strengths of anomaly detection include the ability to identify rare or unusual events, applicability to high-dimensional data, potential for real-time monitoring, and the wide range of applications across various domains. It can also be used for \n",
    "        unsupervised learning tasks, where there's no need for labeled data or explicit definitions of anomalies.\n",
    "\n",
    "        ## Limitations\n",
    "        \n",
    "        The limitations of anomaly detection include its potential for false positives or false negatives, its sensitivity to data quality, its need for domain knowledge or feature engineering, and its potential for overfitting when using complex models. It might also struggle with concept drift or changing data distributions over time.\n",
    "\n",
    "        ## Alternative and Complimentary Options\n",
    "        \n",
    "        Some alternative options to anomaly detection include:\n",
    "        - Change Detection: A technique that identifies changes or shifts within a dataset over time, by comparing data points across \n",
    "        different time intervals or windows.\n",
    "        - Clustering: A technique that groups data points based on their similarity, by minimizing within-cluster variance or maximizing \n",
    "        between-cluster separation. Outliers can be identified as data points that do not belong to any cluster or are far from any cluster.\n",
    "        - Classification: A technique that assigns labels or categories to data points, by constructing a decision boundary that separates \n",
    "        different classes. Outliers can be identified as data points that fall outside this boundary or are misclassified.\n",
    "\n",
    "        ## Learning Resources\n",
    "\n",
    "        - Anomaly detection using Isolation Forest â€“ A Complete Guide https://www.analyticsvidhya.com/blog/2021/07/anomaly-detection-using-isolation-forest-a-complete-guide/\n",
    "        \n",
    "        - PyOD- This open-source Python library offers a variety of anomaly detection algorithms, including statistical, distance-based, density-based, and clustering-based methods. It is a versatile Python library for detecting anomalies in multivariate data. https://pypi.org/project/pyod/\n",
    "        \n",
    "        - Anomaly Detection in Python: Best Practices and Techniques by Dmytro Iakubovskyi. https://medium.com/data-and-beyond/anomaly-detection-in-python-best-practices-and-techniques-9b93d37244dc\n",
    "\n",
    "        - Coursera Course: Unsupervised Learning, Recommenders, Reinforcement Learning. This is a beginner level course focusing on unsupervised learning including clustering and anomaly detection. https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning\n",
    "\n",
    "        - Microsoft article on using anomaly detection in cyber security: DETECTING CYBER ATTACKS USING ANOMALY DETECTION WITH\n",
    "        EXPLANATIONS AND EXPERT FEEDBACK https://www.microsoft.com/en-us/research/uploads/prod/2019/06/ADwithGraderFeedback.pdf\n",
    "\n",
    "        - Kaggle Competition: Anomaly Detection\\*\\* This Kaggle competition provides a real-world dataset for anomaly detection in financial transactions. Participants can develop and submit their own anomaly detection models and compete for prizes. https://www.kaggle.com/c/ieee-fraud-detection \n",
    "\n",
    "        - Scikit-learn: Anomaly Detection\\*\\* The scikit-learn library includes several anomaly detection algorithms, such as Isolation Forest, Local Outlier Factor (LOF), and One-Class Support Vector Machines (OCSVM). These algorithms are easy to implement and can be used for a wide range of anomaly detection tasks. https://scikit-learn.org/stable/modules/outlier_detection.html\n",
    "\n",
    "        ## Key Terms\n",
    "\n",
    "        - One-class SVM\n",
    "        - Isolation Forest\n",
    "        - Autoencoders\n",
    "        - Control Charts\n",
    "        - Network Security\n",
    "        - Outlier Detection\n",
    "        - Clustering\n",
    "        - Change Detection\n",
    "    \"\"\"\n",
    "    },\n",
    "    {\"question\": \"\"\"Create an educational article that a data scientist with some college education can use \n",
    "        to teach themselves about Automated Knowledge Graph Construction. Please answer the following questions with each section\n",
    "        of the response:\n",
    "        Explain the core principles or components and brief history of Automated Knowledge Graph Construction..\n",
    "        What are some applications where a data scientist can use Automated Knowledge Graph Construction?\n",
    "        When Should Automated Knowledge Graph Construction be utilized?\n",
    "        What specific technologies should a data scientist focus on in order to become an\n",
    "        expert in Automated Knowledge Graph Construction? Please include specific technologies by name, along with where\n",
    "        a data scientist can learn more about these specific technologies. \n",
    "        authoritative institutions that are important to Automated Knowledge Graph Construction.\n",
    "        What are the strengths of Automated Knowledge Graph Construction?\n",
    "        What are the Limitations of Automated Knowledge Graph Construction?\n",
    "        What are Alternative Options to Automated Knowledge Graph Construction?\n",
    "        List the most common terminology associated with Automated Knowledge Graph Construction and give a brief definition for \n",
    "        each.\n",
    "        What are some Example Deployments of Automated Knowledge Graph Construction?\n",
    "        Please list some beginner level and intermediate level resources for someone learning \n",
    "        about Automated Knowledge Graph Construction to use in order to gain expertise and background knowledge. \n",
    "    \n",
    "     \"\"\",\n",
    "        \"answer\": \"\"\"Automated Knowledge Graph Construction\n",
    "\n",
    "        ## Overview\n",
    "\n",
    "        Automated Knowledge Graph Construction (AKGC) is a subfield of Artificial Intelligence (AI) that focuses on automatically constructing knowledge graphs from various data sources. A knowledge graph is a graph-based data structure that represents entities and their relationships, making it easier to analyze, reason, and draw inferences from complex data sets. AKGC involves several components, including data preprocessing, feature extraction, graph generation, and evaluation.\n",
    "\n",
    "        The history of AKGC can be traced back to the early days of AI, with researchers exploring techniques for automated reasoning, inference, and knowledge representation. However, it was not until the advent of big data and the internet that AKGC gained significant traction, enabling organizations to manage and make sense of vast amounts of information.\n",
    "\n",
    "        ## Applications\n",
    "\n",
    "        AKGC has numerous applications across various domains, including:\n",
    "\n",
    "        - Cybersecurity: Constructing knowledge graphs from security data can help identify attack patterns, visualize threats, and automate incident response.\n",
    "        - Healthcare: AKGC can be used to create knowledge graphs from electronic health records, enabling clinicians to make more informed decisions about patient care.\n",
    "        - Finance: AKGC can help financial institutions detect fraud, assess credit risk, and automate compliance processes.\n",
    "        - Natural Language Processing (NLP): AKGC can be used to extract meaning from unstructured text data, enabling applications like chatbots, sentiment analysis, and document classification.\n",
    "        - E-commerce: AKGC can help retailers analyze customer behavior, personalize recommendations, and automate supply chain management.\n",
    "        - Smart Cities: AKGC can be used to manage urban infrastructure, optimize resource allocation, and improve public safety.\n",
    "\n",
    "        ## When to use AKGC\n",
    "\n",
    "        AKGC should be utilized when dealing with large volumes of data that require complex analysis, reasoning, and inference. It is particularly useful for tasks that involve:\n",
    "        - Identifying patterns and relationships across different data sources.\n",
    "        - Automating repetitive tasks, such as data cleaning, and feature extraction.\n",
    "        - Enhancing decision-making processes with context-rich visualizations.\n",
    "        - Supporting real-time analysis and prediction tasks.\n",
    "        - Facilitating interdisciplinary collaboration by integrating domain-specific knowledge with data-driven insights.\n",
    "\n",
    "        ## Technologies\n",
    "\n",
    "        To become an expert in AKGC, a data scientist should focus on the following technologies:\n",
    "        - Graph Databases: Neo4j, OrientDB, and Titan are popular graph databases that support knowledge graph construction and analysis.\n",
    "        - Semantic Web Technologies: RDF (Resource Description Framework) and SPARQL (SPARQL Protocol and RDF Query Language) are essential for representing entities, relationships, and queries within a knowledge graph.\n",
    "        - Machine Learning (ML): Techniques like clustering, classification, and deep learning can be used to extract features from data, enabling more accurate knowledge graph generation.\n",
    "        - Natural Language Processing (NLP): Techniques like named entity recognition, sentiment analysis, and text classification can be used to extract meaning from unstructured text data, enriching knowledge graphs with contextual information.\n",
    "        - Programming Languages: Python, Java, and JavaScript are popular programming languages for developing AKGC applications, with libraries like Py2neo, Janus, and GraphQL providing support for graph database operations and semantic web technologies.\n",
    "\n",
    "        ## Strengths and Limitations\n",
    "\n",
    "        Strengths of AKGC include its ability to handle large volumes of data, support real-time analysis, and facilitate interdisciplinary collaboration. However, it also has some limitations, such as data quality issues, performance challenges, and the need for expertise in various domains. Additionally, AKGC may not be widely adopted in some organizations due to concerns about data privacy, security, and ownership.\n",
    "\n",
    "        ## Alternative Options\n",
    "\n",
    "        Alternative options to AKGC include manual knowledge graph construction, rule-based systems, and traditional data analysis techniques like regression, clustering, and classification. However, these methods may not scale well with big data or provide the same level of context-awareness as AKGC.\n",
    "\n",
    "        ## Terminology\n",
    "\n",
    "        Some common terminology associated with AKGC includes:\n",
    "        - Knowledge Graph: A graph-based data structure that represents entities and their relationships.\n",
    "        - Entity: A thing or concept within a knowledge graph, such as a person, organization, or location.\n",
    "        - Relationship: A connection between two entities, representing a type of interaction or association (e.g., \"works at,\" \"attended,\" \"located at\").\n",
    "        - Feature Extraction: The process of extracting relevant features from data sources, enabling more accurate knowledge graph generation.\n",
    "        - Graph Generation: The process of creating a knowledge graph from data sources, often involving feature extraction, entity recognition, and relationship identification.\n",
    "        - Evaluation: The process of assessing the quality, accuracy, and completeness of a knowledge graph, often using metrics like precision, recall, and F1 score.\n",
    "        - Semantic Web: A vision for the web where data is represented using standardized formats like RDF, enabling machines to process and reason about information more effectively.\n",
    "        - Big Data: Large volumes of data that require specialized tools and techniques for processing, analysis, and visualization.\n",
    "        - Machine Learning: A subfield of AI that focuses on developing algorithms for pattern recognition, prediction, and decision-making.\n",
    "        - Natural Language Processing: A subfield of AI that focuses on enabling machines to understand, interpret, and generate human language.\n",
    "        - Cybersecurity: The practice of protecting computer systems, networks, and sensitive information from unauthorized access, attack, or damage.\n",
    "        - Smart Cities: The field of urban planning focused on using technology to improve public services, enhance quality of life, and reduce environmental impact.\n",
    "\n",
    "        ## Deployments\n",
    "\n",
    "        Example deployments of AKGC include:\n",
    "        - Merative (formerly IBM Watson Health) for analyzing electronic health records and improving patient outcomes: https://www.merative.com/company\n",
    "        - Amazon Neptune for managing graph databases and supporting knowledge graph applications: https://aws.amazon.com/neptune/\n",
    "        - Microsoft Azure Cosmos DB for indexing and querying graph-based data models: https://azure.microsoft.com/en-us/services/cosmos-db/\n",
    "\n",
    "        ## Resources\n",
    "\n",
    "        To learn more about Automated Knowledge Graph Construction:\n",
    "        - Tutorials: Websites like Towards Data Science, Analytics Vidhya, and Machine Learning Mastery offer tutorials on AKGC, covering topics like feature extraction, graph generation, and evaluation.\n",
    "        - Books: Books like \"Graph Databases\" by Ian Robinson provide comprehensive overviews of AKGC, including its history, principles, and applications.\n",
    "\n",
    "        ## Additional Resources:\n",
    "\n",
    "        1. DeepDive: A Data-Driven Knowledge Graph Construction System: https://www.cs.cornell.edu/~cdesa/papers/sigmodrecord2016_deepdive_highlight.pdf\n",
    "        DeepDive is a system for automatically constructing knowledge graphs from unstructured text. It uses a variety of natural language processing and machine learning techniques to extract and link information from text.\n",
    "        2. Neural Knowledge Graph Embeddings for Link Prediction: https://arxiv.org/pdf/2008.07723.pdf\n",
    "        This paper introduces a neural network-based approach to knowledge graph embedding. It shows that neural embeddings can capture complex relationships between entities and improve the accuracy of link prediction tasks.\n",
    "        3. A Comprehensive Survey on Automatic Knowledge Graph\n",
    "        Construction: https://dl.acm.org/doi/pdf/10.1145/3618295\n",
    "        This paper is a survey of more than 300 methods to summarize\n",
    "        the latest developments in knowledge graph construction.\n",
    "        4. COMET: Commonsense Transformers for Automatic Knowledge Graph Construction: https://aclanthology.org/P19-1470.pdf\n",
    "        This article covers the development of Commonsense Transformers\n",
    "        for Automatic Knowledge Graph Construction which aim to create generative models that learn to generate rich and diverse commonsense descriptions in natural language.\n",
    "\n",
    "        ## Key Terms\n",
    "        - Py2neo\n",
    "        - GraphQL\n",
    "        - Neo4j\n",
    "        - OrientDB\n",
    "        - Knowledge Graph\n",
    "        - Graph Databases\n",
    "        - Watson\n",
    "        - Artificial Intelligence\n",
    "        - Java\n",
    "        - Azure\n",
    "        - Titan\n",
    "        - Semantic Web Technologies\n",
    "        - Graph Generation\n",
    "        - SPARQL\n",
    "        - F1\n",
    "        - IBM Watson Health\n",
    "        - Merative\n",
    "        - Feature Extraction\n",
    "        - Neptune\n",
    "        - NLP\n",
    "        - Ian Robinson\n",
    "        - OrientDB\n",
    "        - JavaScript\n",
    "        - RDF\n",
    "        - Microsoft Azure Cosmos\n",
    "        - KDnuggets\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"\"\"Create an educational article that a person with some college education can use \n",
    "        to teach themselves about cybersecurity knowledge graphs. Please answer the following questions with each section of the answer:\n",
    "        What Is cybersecurity knowledge graphs?\n",
    "        Who Should Use cybersecurity knowledge graphs?\n",
    "        When Should cybersecurity knowledge graphs be utilized?\n",
    "        How Does a person learn about cybersecurity knowledge graphs? Please include specific steps for a person who is starting to learn about the topic, such as information about technology solutions or authoritative institutions that are important to cybersecurity knowledge graphs.\n",
    "        What are the strengths of cybersecurity knowledge graphs?\n",
    "        What are the Limitations of cybersecurity knowledge graphs?\n",
    "        What are Alternative Options to cybersecurity knowledge graphs?\n",
    "        What are some Example Deployments of cybersecurity knowledge graphs?\n",
    "        For each of the items above, Include two to three specific facts or statements that support the general points you make. Please provide the answer in markdown format.\n",
    "\"\"\",\n",
    "        \"answer\":\"\"\"# Cybersecurity Knowledge Graphs\n",
    "        \n",
    "        ## Overview:\n",
    "\n",
    "        Cybersecurity Knowledge Graphs (CKGs) are a type of knowledge representation that uses graph structures to represent and organize information about cybersecurity threats, vulnerabilities, and countermeasures. They visually depict entities and relationships between them in a graphical structure, making it easier to understand and reason about cybersecurity incidents. By leveraging semantic technologies, CKGs enable context-aware analysis, helping security professionals make informed decisions. As with any knowledge graph, CKGs generally consist of entities (nodes) and relationships (edges). Common entities in CKGs are:\n",
    "        Threats: Information about different cyber attacks, malware, and hacking techniques.\n",
    "        Vulnerabilities: Weaknesses in systems and software that attackers can exploit.\n",
    "        Systems: Devices, applications, and networks within an organization.\n",
    "        Indicators of Compromise (IOCs): Clues that a system might be under attack.\n",
    "        By connecting these elements, the knowledge graph can reveal hidden patterns and connections. Here are some examples of how this works:idents, especially those involving multiple entities or relationships. They are particularly helpful for:\n",
    "\n",
    "        - Identifying attack paths: Analysts can see how attackers might move from one vulnerability in a system to another, ultimately reaching sensitive data.\n",
    "        - Threat intelligence: The knowledge graph can connect information about new threats with similar attacks in the past, helping security teams anticipate and respond faster.\n",
    "        - Incident investigation: Security professionals can use the knowledge graph to link specific IOCs with known threats, speeding up investigations.\n",
    "\n",
    "        ## Applications and Use Cases\n",
    "\n",
    "        CKGs are valuable for cybersecurity analysts who need to manage large volumes of security data, identify patterns, and perform root cause analysis. They can also benefit CTI (Cyber Threat Intelligence) teams by providing context-rich visualizations that aid in understanding evolving threats and their relationships with various entities in the cybersecurity landscape.\n",
    "\n",
    "        ## When are they needed?\n",
    "        \n",
    "        idents, especially those involving multiple entities or relationships. They are particularly helpful for:\n",
    "\n",
    "        - Analyzing large-scale data breaches\n",
    "        - Uncovering sophisticated attack patterns\n",
    "        - Performing root cause analysis of security incidents\n",
    "        - Visualizing and understanding threat intelligence\n",
    "        - Supporting security automation and orchestration initiatives\n",
    "\n",
    "        ## Focus Areas\n",
    "\n",
    "        Learning about CKGs involves understanding their underlying technologies, applications, and best practices. Key steps include:\n",
    "\n",
    "        - Familiarize yourself with graph databases, such as Neo4j, and the principles of knowledge representation in graphs.\n",
    "        - Study semantic web technologies, including RDF (Resource Description Framework) and SPARQL (SPARQL Protocol and RDF Query Language), to learn how to represent entities and relationships effectively.\n",
    "        - Explore existing CKG solutions, like MITRE's ATT&CK knowledge graph, to understand real-world applications and data models.\n",
    "        - Practice using tools and platforms that enable the creation, management, and analysis of CKGs. Online courses, workshops, and tutorials can help you gain hands-on experience.\n",
    "\n",
    "        ## Strengths\n",
    "\n",
    "        CKGs offer several advantages:\n",
    "\n",
    "        - Context-aware analysis: By representing relationships between entities, CKGs provide a more comprehensive understanding of cybersecurity incidents.\n",
    "        - Scalability: CKGs can handle large volumes of data and grow as new information becomes available.\n",
    "        - They allow for the identification of relationships and dependencies between different security components.\n",
    "        - Query flexibility: graph data systems like Neo4j and SPARQL offer powerful querying capabilities to extract insights from the graph structure.\n",
    "        - Real-time analysis: CKGs can be updated in real time, making them suitable for incident response scenarios.\n",
    "        - Enhanced Inference Capabilities: They enable the identification of new relationships and patterns not directly observed in the data due to their inherent interpretability of semantic connections.\n",
    "\n",
    "        ## Weaknesses\n",
    "\n",
    "        Despite their benefits, CKGs have some limitations:\n",
    "\n",
    "        - Complexity: Building and maintaining a CKG can be challenging, requiring expertise in various domains (e.g., graph databases, semantic web technologies).\n",
    "        - Data quality: The accuracy and completeness of the data used to build the CKG significantly impact its usefulness.\n",
    "        - Performance: Querying large graphs may lead to performance issues if not properly optimized.\n",
    "        - Adoption: Although gaining traction, CKGs are not yet widely adopted in cybersecurity operations, which could limit access to resources and best practices.\n",
    "        - Skill Acquisition: The interdisciplinary nature of the subject requires proficiency in cybersecurity, data modeling, graph theory, and semantic web technologies.\n",
    "\n",
    "        ## Alternative and Complimentary Options\n",
    "\n",
    "        Other options for visualizing and analyzing cybersecurity data include:\n",
    "\n",
    "        - Security Information and Event Management (SIEM) systems.\n",
    "        - Security Orchestration, Automation, and Response (SOAR) platforms.\n",
    "        - Threat intelligence platforms (TIPs)\n",
    "\n",
    "        ## Example Deployments\n",
    "        \n",
    "        Example Deployments of Cybersecurity Knowledge Graphs CKGs have been successfully deployed in various cybersecurity applications:\n",
    "\n",
    "        - Security Automation: KGs can be used to automate security tasks like incident response. By linking indicators of compromise (IOCs) to known threats within the knowledge graph, security systems can automatically trigger predefined responses when an IOC is detected, saving time and improving efficiency.\n",
    "\n",
    "\n",
    "        - Security Configuration Management: KGs can be used to ensure secure configurations for complex systems like container orchestration platforms (e.g., Kubernetes). By storing information about secure configuration parameters and their relationships, the knowledge graph can identify misconfigurations and recommend fixes, preventing potential security vulnerabilities.\n",
    "\n",
    "        - Threat Intelligence Analysis: KGs can be incredibly useful for threat analysts. They can connect information about new threats with past attacks and similar malware strains. This allows analysts to understand the broader context of a threat, predict its behavior, and develop more effective mitigation strategies.\n",
    "\n",
    "        - MITRE's ATT&CK knowledge graph for mapping adversary tactics and techniques: https://attack.mitre.org\n",
    "\n",
    "        - IBM's X-Force Threat Intelligence Platform, which uses a graph database to represent threat data: https://exchange.xforce.ibmcloud.com\n",
    "\n",
    "        - NIST Cybersecurity Framework for understanding and managing cybersecurity risk: https://www.nist.gov/cyberframework\n",
    "        \"\"\"\n",
    "    },]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below define our question or prompt that we will feed to the LLM and we build a function for picking the most similar example from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "def example_picker(embeddings,n_samples):\n",
    "    example_sel = SemanticSimilarityExampleSelector.from_examples(\n",
    "                                examples,\n",
    "                                embeddings,\n",
    "                                Chroma,\n",
    "                                # Number of examples\n",
    "                                k=n_samples,\n",
    "                                )\n",
    "    return example_sel\n",
    "\n",
    "def ko_question_func(subject):\n",
    "    question = f\"\"\"\n",
    "            Create an educational article that a data scientist with some college education can use \n",
    "            to teach themselves about {subject}. Please answer the following questions with each section\n",
    "            of the response:\n",
    "            Explain the core principles or components and brief history of {subject}.\n",
    "            What are some applications where a data scientist can use {subject}?\n",
    "            When Should {subject} be utilized?\n",
    "            What specific technologies should a data scientist focus on in order to become an\n",
    "            expert in {subject}? Please include specific technologies by name, along with where\n",
    "            a data scientist can learn more about these specific technologies. \n",
    "            What authoritative institutions are important to {subject}.\n",
    "            What are the strengths of {subject}?\n",
    "            What are the Limitations of {subject}?\n",
    "            What are Alternative Options to {subject}?\n",
    "            List the most common terminology associated with {subject} and give a brief definition for \n",
    "            each.\n",
    "            What are some Example Deployments of {subject}?\n",
    "\t        Please list some beginner level and intermediate level resources for someone learning \n",
    "            about {subject} to use in order to gain expertise and background knowledge.\n",
    "            \"\"\"\n",
    "\n",
    "    return question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Create an educational article that a data scientist with some college education can use \n",
      "        to teach themselves about Anomaly Detection. Please answer the following questions with each section\n",
      "        of the response:\n",
      "        Explain the core principles or components and brief history of Anomaly Detection..\n",
      "        What are some applications where a data scientist can use Anomaly Detection?\n",
      "        When Should Anomaly Detection be utilized?\n",
      "        What specific technologies should a data scientist focus on in order to become an\n",
      "        expert in Anomaly Detection? Please include specific technologies by name, along with where\n",
      "        a data scientist can learn more about these specific technologies. \n",
      "        authoritative institutions that are important to Anomaly Detection.\n",
      "        What are the strengths of Anomaly Detection?\n",
      "        What are the Limitations of Anomaly Detection?\n",
      "        What are Alternative Options to Anomaly Detection?\n",
      "        List the most common terminology associated with Anomaly Detection and give a brief definition for \n",
      "        each.\n",
      "        What are some Example Deployments of Anomaly Detection?\n",
      "        Please list some beginner level and intermediate level resources for someone learning \n",
      "        about Anomaly Detection to use in order to gain expertise and background knowledge. \n",
      "    \n",
      "AI: # Anomaly Detection\n",
      "\n",
      "\n",
      "        ## Summary of Anomaly Detection:\n",
      "        \n",
      "        Anomaly detection is a technique used by data scientists to identify unusual or rare events, outliers, or patterns \n",
      "        within a dataset. It involves identifying deviations from normal behavior or expected outcomes, which can be useful \n",
      "        for detecting fraud, diagnosing diseases, monitoring network security, and more. The field has its roots in statistical \n",
      "        methods like control charts, clustering, and outlier detection, with recent advancements driven by machine learning \n",
      "        techniques like one-class SVM, isolation forests, and autoencoders.\n",
      "\n",
      "        ## Applications :\n",
      "        \n",
      "        Anomaly detection has a wide range of applications across various domains, including:\n",
      "        - Finance: detecting fraudulent transactions, identifying insider trading, or predicting stock market crashes.\n",
      "        - Healthcare: diagnosing diseases, monitoring patient health, or detecting medical equipment failures.\n",
      "        - Retail: identifying shoplifting, preventing inventory theft, or detecting supply chain disruptions.\n",
      "        - Cybersecurity: detecting intrusions, identifying malware, or monitoring network traffic for anomalies.\n",
      "        - Manufacturing: detecting equipment failures, monitoring product quality, or predicting maintenance needs.\n",
      "        - Environmental monitoring: detecting pollution, monitoring climate change, or predicting natural disasters.\n",
      "\n",
      "        ## When Should Anomaly Detection be utilized?\n",
      "        \n",
      "        Anomaly detection should be utilized when there's a need to identify rare or unusual events within a dataset, or when \n",
      "        there's a need to monitor system behavior for deviations from expected norms. It's particularly useful when dealing \n",
      "        with high-dimensional data, where traditional statistical methods might not be applicable or efficient.\n",
      "\n",
      "        ## Focus Areas\n",
      "        \n",
      "        A data scientist should focus on learning about machine learning techniques like one-class SVM, isolation forests, autoencoders, \n",
      "        as well as statistical methods like control charts, clustering, and outlier detection. Some specific technologies include:\n",
      "        - One-class SVM (Support Vector Machine): A technique that can detect outliers by constructing a hyperplane that \n",
      "        separates data points from a \"pseudo-outlier\" class.\n",
      "        - Isolation Forests: A method that detects anomalies by growing multiple decision trees on random subsets of data, \n",
      "        and identifying deviant instances based on their unique feature subsets.\n",
      "        - Autoencoders: A neural network architecture that learns data representations by encoding input data into a lower-dimensional \n",
      "        space, and decoding it back to its original form. Anomalies can be detected by comparing the encoded data with its reconstruction.\n",
      "        - Control Charts: A statistical method that monitors the behavior of a process variable over time, by constructing \n",
      "        moving averages and standard deviations, and identifying deviations from expected norms.\n",
      "        - Clustering: A technique that groups data points based on their similarity, by minimizing within-cluster variance or maximizing \n",
      "        between-cluster separation. Outliers can be identified as data points that do not belong to any cluster or are far from any cluster.\n",
      "        - Outlier Detection: A method that identifies outliers by computing summary statistics like mean, median, or standard deviation, \n",
      "        and identifying data points that deviate significantly from these norms.\n",
      "\n",
      "        ## Strengths\n",
      "        \n",
      "        The strengths of anomaly detection include the ability to identify rare or unusual events, applicability to high-dimensional data, potential for real-time monitoring, and the wide range of applications across various domains. It can also be used for \n",
      "        unsupervised learning tasks, where there's no need for labeled data or explicit definitions of anomalies.\n",
      "\n",
      "        ## Limitations\n",
      "        \n",
      "        The limitations of anomaly detection include its potential for false positives or false negatives, its sensitivity to data quality, its need for domain knowledge or feature engineering, and its potential for overfitting when using complex models. It might also struggle with concept drift or changing data distributions over time.\n",
      "\n",
      "        ## Alternative and Complimentary Options\n",
      "        \n",
      "        Some alternative options to anomaly detection include:\n",
      "        - Change Detection: A technique that identifies changes or shifts within a dataset over time, by comparing data points across \n",
      "        different time intervals or windows.\n",
      "        - Clustering: A technique that groups data points based on their similarity, by minimizing within-cluster variance or maximizing \n",
      "        between-cluster separation. Outliers can be identified as data points that do not belong to any cluster or are far from any cluster.\n",
      "        - Classification: A technique that assigns labels or categories to data points, by constructing a decision boundary that separates \n",
      "        different classes. Outliers can be identified as data points that fall outside this boundary or are misclassified.\n",
      "\n",
      "        ## Learning Resources\n",
      "\n",
      "        - Anomaly detection using Isolation Forest â€“ A Complete Guide https://www.analyticsvidhya.com/blog/2021/07/anomaly-detection-using-isolation-forest-a-complete-guide/\n",
      "        \n",
      "        - PyOD- This open-source Python library offers a variety of anomaly detection algorithms, including statistical, distance-based, density-based, and clustering-based methods. It is a versatile Python library for detecting anomalies in multivariate data. https://pypi.org/project/pyod/\n",
      "        \n",
      "        - Anomaly Detection in Python: Best Practices and Techniques by Dmytro Iakubovskyi. https://medium.com/data-and-beyond/anomaly-detection-in-python-best-practices-and-techniques-9b93d37244dc\n",
      "\n",
      "        - Coursera Course: Unsupervised Learning, Recommenders, Reinforcement Learning. This is a beginner level course focusing on unsupervised learning including clustering and anomaly detection. https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning\n",
      "\n",
      "        - Microsoft article on using anomaly detection in cyber security: DETECTING CYBER ATTACKS USING ANOMALY DETECTION WITH\n",
      "        EXPLANATIONS AND EXPERT FEEDBACK https://www.microsoft.com/en-us/research/uploads/prod/2019/06/ADwithGraderFeedback.pdf\n",
      "\n",
      "        - Kaggle Competition: Anomaly Detection\\*\\* This Kaggle competition provides a real-world dataset for anomaly detection in financial transactions. Participants can develop and submit their own anomaly detection models and compete for prizes. https://www.kaggle.com/c/ieee-fraud-detection \n",
      "\n",
      "        - Scikit-learn: Anomaly Detection\\*\\* The scikit-learn library includes several anomaly detection algorithms, such as Isolation Forest, Local Outlier Factor (LOF), and One-Class Support Vector Machines (OCSVM). These algorithms are easy to implement and can be used for a wide range of anomaly detection tasks. https://scikit-learn.org/stable/modules/outlier_detection.html\n",
      "\n",
      "        ## Key Terms\n",
      "\n",
      "        - One-class SVM\n",
      "        - Isolation Forest\n",
      "        - Autoencoders\n",
      "        - Control Charts\n",
      "        - Network Security\n",
      "        - Outlier Detection\n",
      "        - Clustering\n",
      "        - Change Detection\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\",\"{question}\"), (\"ai\",\"{answer}\")],\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info on creating a few shot template: https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Set up the LLMs\n",
    "\n",
    "This section contains the code to set up several LLMs to compare results and find the best option. We can use any of these options for the KO creation and for the resource link request step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Party Hosted LLMs\n",
    "\n",
    "These LLMs are hosted by outside companies like Google, OpenAI and the like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google LLMs\n",
    "\n",
    "As of Spring 2024 Google Gemini Pro could be used with out cost. I believe this is set to change soon so I'm not sure how this will work going forward. During Spring 24 I found this to be the most cost effective and the results were OK, not the best but workable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gemini pro can be used with an API key without a premium account. There are some limitations.\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "gg_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "gg_model = GoogleGenerativeAI(model=\"gemini-pro\", \n",
    "                           google_api_key=gg_key,\n",
    "                           temperature=0.2, \n",
    "                           num_words=1000, \n",
    "                           convert_system_message_to_human=True, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info on setting up google genai:\n",
    "\n",
    "https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Anthropic Claude 3\n",
    "\n",
    "Overall I found Claude 3 to be the best paid option. The cost for the version listed below was about 10 cents per API call, and they did offer a $5 credit for first time users at that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anthropic Claude models are pay per use, they currently offer $5 credit free for a new account.\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "chat_claude = ChatAnthropic(anthropic_api_key=anthropic_api_key, \n",
    "                            model_name=\"claude-3-opus-20240229\",\n",
    "                            temperature=0.1,\n",
    "                            verbose=True,\n",
    "                            max_tokens=3000\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Chat GPT\n",
    "\n",
    "GPT 4 as shown below cost about 10 cents per API call. The results were slightly better than Google Gemini Pro and below Claude 3 in my opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "gpt_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "gpt_model = ChatOpenAI(#model_name=\"gpt-3.5-instruct\", \n",
    "                        model_name=\"gpt-4-0125-preview\",\n",
    "                        openai_api_key=gpt_key,\n",
    "                        temperature=0.2,\n",
    "                        model_kwargs={\n",
    "                            \"frequency_penalty\": 0.5\n",
    "                        }\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Huggingface LLM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are running a local llm using huggingface. Running this code will download the model to the local machine which can take time as the model file is typically very large. The models that were most effective with a decently powered PC were under 13 billion parameter, quantized models.\n",
    "\n",
    "For more info on how to set up Huggingface on your system here are some references:\n",
    "\n",
    "[More info about the models](https://huggingface.co/models)\n",
    "\n",
    "[Huggingface Docs](https://huggingface.co/docs/transformers/main/llm_tutorial)\n",
    "\n",
    "[How to Blog](https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "\n",
    "# max context is 32768 \n",
    "\n",
    "hf_pipe = pipeline(\"text-generation\",\n",
    "                    device_map=\"auto\",\n",
    "                    do_sample=True,\n",
    "                    # device=0,\n",
    "                    model=\"meta-llama/model_name_here\",\n",
    "                    trust_remote_code=True,\n",
    "                    max_new_tokens=2000, \n",
    "                    temperature=0.8,\n",
    "                    pad_token_id=\"eos_token_id\",\n",
    "                    )\n",
    "hf = HuggingFacePipeline(pipeline=hf_pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local LLMs using LlamaCpp and GGUF models\n",
    "\n",
    "There are many tutorials for setting up LlamaCpp, but overall this was the system I found to be the most efficient.  Ultimately I found this to be the most effective and simple to set up. Once it's running you will just need to download models in GGUF format and place them in the model folder.\n",
    "\n",
    "Here are a couple good tutorials on setting this up on your system:\n",
    "\n",
    "[Datacamp Tutorial](https://www.datacamp.com/tutorial/llama-cpp-tutorial)\n",
    "\n",
    "[Medium Article](https://medium.com/@fradin.antoine17/3-ways-to-set-up-llama-2-locally-on-cpu-part-1-5168d50795ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model was effective on my local machine, your mileage may vary but typically an 8B to 13B parameter model seems to work fine.\n",
    "\n",
    "# from langchain.llms import LlamaCpp\n",
    "\n",
    "\n",
    "# llm_cpp = LlamaCpp(\n",
    "#             streaming = False,\n",
    "#             model_path=\"/home/pete/models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "#             n_gpu_layers=1,\n",
    "#             n_batch=512,\n",
    "#             temperature=0.1,\n",
    "#             verbose=True,\n",
    "#             n_ctx=32768,\n",
    "#             max_tokens=0,\n",
    "#             max_seq_length = None,\n",
    "#             top_p=0.9,\n",
    "#             top_k=50,\n",
    "#             repetition_penalty=1.2,\n",
    "#             presence_penalty=1.0,\n",
    "#             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pete/Documents/ML_code/mlenv/lib/python3.10/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! max_seq_length is not default parameter.\n",
      "                max_seq_length was transferred to model_kwargs.\n",
      "                Please confirm that max_seq_length is what you intended.\n",
      "  warnings.warn(\n",
      "/home/pete/Documents/ML_code/mlenv/lib/python3.10/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! repetition_penalty is not default parameter.\n",
      "                repetition_penalty was transferred to model_kwargs.\n",
      "                Please confirm that repetition_penalty is what you intended.\n",
      "  warnings.warn(\n",
      "/home/pete/Documents/ML_code/mlenv/lib/python3.10/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! presence_penalty is not default parameter.\n",
      "                presence_penalty was transferred to model_kwargs.\n",
      "                Please confirm that presence_penalty is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /home/pete/models/nous-hermes-llama2-13b.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32032]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32032]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32032]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_0:  281 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 291/32032 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32032\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 6.86 GiB (4.53 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7024.11 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size = 25600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 25600.00 MiB, K (f16): 12800.00 MiB, V (f16): 12800.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    75.26 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2600.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "\n",
    "herm_cpp = LlamaCpp(\n",
    "            streaming = False,\n",
    "            model_path=\"/home/pete/models/nous-hermes-llama2-13b.Q4_0.gguf\",\n",
    "            n_gpu_layers=1,\n",
    "            n_batch=512,\n",
    "            temperature=0.2,\n",
    "            verbose=True,\n",
    "            n_ctx=32768,\n",
    "            max_tokens=0,\n",
    "            max_seq_length = None,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2,\n",
    "            presence_penalty=1.0,\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Ollama server (running on another device on local net)\n",
    "\n",
    "For this method you first need to setup Ollama and then activate and load a model. In my case I was running Ollama on a different device and connecting remotly but if it's a local instance just replace the IP address in the base_url with localhost. Here are some resources for setting up an Ollama instance (I used a docker container and had good success with that.)\n",
    "\n",
    "[Getting Started with Ollama and Docker](https://collabnix.com/getting-started-with-ollama-and-docker/)\n",
    "\n",
    "[Dockerhub Ollama](https://hub.docker.com/r/ollama/ollama#!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "# # from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# ollama_model_2 = Ollama(base_url='http://192.168.68.125:11434',   #your local ollama server address here\n",
    "# model=\"mixtral\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Setup our Knowledge Object Building Function\n",
    "\n",
    "### Generate Knowledge Objects\n",
    "Here we set up the functions to run our knowledge object prompts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The system message below will be added to our prompt and is designed to guide the LLM on how to answer the question.\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "def ko_model_run(embeddings, n_samples, example_prompt, subject, model):\n",
    "    example_sel=example_picker(embeddings,n_samples)\n",
    "    question = ko_question_func(subject)\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_selector=example_sel,\n",
    "        example_prompt=example_prompt,\n",
    "        # suffix=\"Question: {input}\",\n",
    "        input_variables=[\"question\"],\n",
    "        )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "        SystemMessage(\n",
    "            content=(\"\"\"Return the response to the question in markdown format and written in paragraph \n",
    "                     form in the style of an educational blog post preferring complete paragraphs instead of\n",
    "                     lists of bullet points. Please keep the response between 1000 and 1500 words in length.\n",
    "                     When citing references only use real, verifiable references with links that are known to\n",
    "                     exist on the internet now, do not use creativity to generate these items.\n",
    "                     \"\"\"\n",
    "            )\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        HumanMessage(content=question)\n",
    "        ]\n",
    "        )\n",
    "    chain = prompt | model\n",
    "    output = chain.invoke({\"question\":question})\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0 = topic_list['Subject'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   43081.19 ms\n",
      "llama_print_timings:      sample time =     192.45 ms /  1460 runs   (    0.13 ms per token,  7586.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =  220605.72 ms /  2757 tokens (   80.02 ms per token,    12.50 tokens per second)\n",
      "llama_print_timings:        eval time =  524869.32 ms /  1459 runs   (  359.75 ms per token,     2.78 tokens per second)\n",
      "llama_print_timings:       total time =  750032.26 ms /  4216 tokens\n"
     ]
    }
   ],
   "source": [
    "#For a one-off run of the above prompt this is sufficient. Below we will set up a function to run multiple outputs at once.\n",
    "\n",
    "output = ko_model_run(baa, 1, example_prompt, topic0, herm_cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAI: Ollama\\n        ## Overview\\n        Ollama is a subfield of Artificial Intelligence (AI) that focuses on developing algorithms for natural language understanding, generation, and interaction. It involves several components, including speech recognition, language modeling, dialogue management, and machine learning. Ollama has numerous applications across various domains, including virtual assistants, chatbots, language translation, and sentiment analysis.\\n        ## History\\n        The history of Ollama can be traced back to the early days of AI, with researchers exploring techniques for natural language processing, understanding, and generation. However, it was not until the advent of big data and the internet that Ollama gained significant traction, enabling organizations to manage and make sense of vast amounts of information.\\n        ## Applications\\n        Ollama has numerous applications across various domains, including:\\n        - Virtual Assistants: Ollama can be used to create virtual assistants that can understand and respond to natural language queries, enabling users to interact with devices more naturally.\\n        - Chatbots: Ollama can be used to develop chatbots that can understand and generate natural language responses, enabling businesses to automate customer service, sales, and marketing tasks.\\n        - Language Translation: Ollama can be used to develop machine translation systems that can translate text from one language to another, enabling people to communicate across linguistic barriers.\\n        - Sentiment Analysis: Ollama can be used to analyze text data and extract sentiment, enabling businesses to understand customer feedback, monitor social media, and improve brand reputation.\\n        - Speech Recognition: Ollama can be used to develop speech recognition systems that can transcribe spoken language into text, enabling people with disabilities to communicate more easily and enabling businesses to automate call centers.\\n        ## When to use Ollama\\n        Ollama should be utilized when dealing with natural language data that requires complex analysis, understanding, and generation. It is particularly useful for tasks that involve:\\n        - Identifying patterns and relationships across different languages.\\n        - Automating customer service tasks, such as answering FAQs, resolving issues, and providing product recommendations.\\n        - Enhancing language translation tasks with context-rich data.\\n        - Supporting real-time language processing tasks, such as sentiment analysis, language detection, and intent recognition.\\n        - Facilitating interdisciplinary collaboration by integrating domain-specific knowledge with language-specific insights.\\n        ## Technologies\\n        To become an expert in Ollama, a data scientist should focus on the following technologies:\\n        - Natural Language Processing (NLP) Libraries: Libraries like NLTK, spaCy, and Gensim provide support for text preprocessing, feature extraction, and language modeling tasks.\\n        - Speech Recognition Libraries: Libraries like SpeechRecognition, PyAudio, and Festival provide support for speech recognition tasks, enabling more natural language interactions with devices.\\n        - Dialogue Management Systems: Systems like Dialogflow, Rasa, and Tars provide support for dialogue management tasks, enabling more engaging and personalized conversations with users.\\n        - Machine Learning Frameworks: Frameworks like TensorFlow, PyTorch, and Scikit-learn provide support for machine learning tasks, enabling more accurate language processing models.\\n        ## Strengths and Limitations\\n        Strengths of Ollama include its ability to handle complex language tasks, support real-time language processing, and facilitate interdisciplinary collaboration. However, it also has some limitations, such as data quality issues, performance challenges, and the need for expertise in various domains. Additionally, Ollama may not be widely adopted in some organizations due to concerns about data privacy, security, and ownership.\\n        ## Alternative Options\\n        Alternative options to Ollama include rule-based systems, statistical language models, and traditional data analysis techniques like regression, clustering, and classification. However, these methods may not scale well with big data or provide the same level of context-awareness as Ollama.\\n        ## Terminology\\n        Some common terminology associated with Ollama includes:\\n        - Natural Language Processing (NLP): A subfield of AI that focuses on enabling machines to understand, interpret, and generate human language.\\n        - Speech Recognition: A subfield of NLP that focuses on developing algorithms for transcribing spoken language into text.\\n        - Dialogue Management: A subfield of NLP that focuses on developing algorithms for managing conversations with users, enabling more engaging and personalized interactions.\\n        - Language Modeling: A subfield of NLP that focuses on developing algorithms for predicting the next word or sentence in a sequence, enabling more accurate language generation tasks.\\n        - Machine Learning (ML): A subfield of AI that focuses on developing algorithms for pattern recognition, prediction, and decision-making.\\n        - Big Data: Large volumes of data that require specialized tools and techniques for processing, analysis, and visualization.\\n        - Sentiment Analysis: A subfield of NLP that focuses on analyzing text data and extracting sentiment, enabling businesses to understand customer feedback, monitor social media, and improve brand reputation.\\n        ## Deployments\\n        Example deployments of Ollama include:\\n        - Amazon Lex for building conversational interfaces that can understand and respond to natural language queries: https://aws.amazon.com/lex/\\n        - Google Cloud Natural Language API for analyzing text data and extracting features like sentiment, syntax, and entity recognition: https://cloud.google.com/natural-language/\\n        - Microsoft Azure Cognitive Services for developing language processing applications that can understand, generate, and interact with natural language data: https://azure.microsoft.com/en-us/services/cognitive-services/\\n        ## Resources\\n        To learn more about Ollama, a data scientist can refer to the following resources:\\n        - Tutorials: Websites like Towards Data Science, Analytics Vidhya, and Machine Learning Mastery offer tutorials on Ollama, covering topics like speech recognition, language modeling, and dialogue management.\\n        - Books: Books like \"Natural Language Processing with Python\" by Michael J. C. Cooper provide comprehensive overviews of Ollama, including its history, principles, and applications.\\n        - Authoritative Institutions: Institutions like ACL, EMNLP, and NAACL provide authoritative resources for Ollama, including conferences, workshops, and journals.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "AI: Ollama\n",
       "        ## Overview\n",
       "        Ollama is a subfield of Artificial Intelligence (AI) that focuses on developing algorithms for natural language understanding, generation, and interaction. It involves several components, including speech recognition, language modeling, dialogue management, and machine learning. Ollama has numerous applications across various domains, including virtual assistants, chatbots, language translation, and sentiment analysis.\n",
       "        ## History\n",
       "        The history of Ollama can be traced back to the early days of AI, with researchers exploring techniques for natural language processing, understanding, and generation. However, it was not until the advent of big data and the internet that Ollama gained significant traction, enabling organizations to manage and make sense of vast amounts of information.\n",
       "        ## Applications\n",
       "        Ollama has numerous applications across various domains, including:\n",
       "        - Virtual Assistants: Ollama can be used to create virtual assistants that can understand and respond to natural language queries, enabling users to interact with devices more naturally.\n",
       "        - Chatbots: Ollama can be used to develop chatbots that can understand and generate natural language responses, enabling businesses to automate customer service, sales, and marketing tasks.\n",
       "        - Language Translation: Ollama can be used to develop machine translation systems that can translate text from one language to another, enabling people to communicate across linguistic barriers.\n",
       "        - Sentiment Analysis: Ollama can be used to analyze text data and extract sentiment, enabling businesses to understand customer feedback, monitor social media, and improve brand reputation.\n",
       "        - Speech Recognition: Ollama can be used to develop speech recognition systems that can transcribe spoken language into text, enabling people with disabilities to communicate more easily and enabling businesses to automate call centers.\n",
       "        ## When to use Ollama\n",
       "        Ollama should be utilized when dealing with natural language data that requires complex analysis, understanding, and generation. It is particularly useful for tasks that involve:\n",
       "        - Identifying patterns and relationships across different languages.\n",
       "        - Automating customer service tasks, such as answering FAQs, resolving issues, and providing product recommendations.\n",
       "        - Enhancing language translation tasks with context-rich data.\n",
       "        - Supporting real-time language processing tasks, such as sentiment analysis, language detection, and intent recognition.\n",
       "        - Facilitating interdisciplinary collaboration by integrating domain-specific knowledge with language-specific insights.\n",
       "        ## Technologies\n",
       "        To become an expert in Ollama, a data scientist should focus on the following technologies:\n",
       "        - Natural Language Processing (NLP) Libraries: Libraries like NLTK, spaCy, and Gensim provide support for text preprocessing, feature extraction, and language modeling tasks.\n",
       "        - Speech Recognition Libraries: Libraries like SpeechRecognition, PyAudio, and Festival provide support for speech recognition tasks, enabling more natural language interactions with devices.\n",
       "        - Dialogue Management Systems: Systems like Dialogflow, Rasa, and Tars provide support for dialogue management tasks, enabling more engaging and personalized conversations with users.\n",
       "        - Machine Learning Frameworks: Frameworks like TensorFlow, PyTorch, and Scikit-learn provide support for machine learning tasks, enabling more accurate language processing models.\n",
       "        ## Strengths and Limitations\n",
       "        Strengths of Ollama include its ability to handle complex language tasks, support real-time language processing, and facilitate interdisciplinary collaboration. However, it also has some limitations, such as data quality issues, performance challenges, and the need for expertise in various domains. Additionally, Ollama may not be widely adopted in some organizations due to concerns about data privacy, security, and ownership.\n",
       "        ## Alternative Options\n",
       "        Alternative options to Ollama include rule-based systems, statistical language models, and traditional data analysis techniques like regression, clustering, and classification. However, these methods may not scale well with big data or provide the same level of context-awareness as Ollama.\n",
       "        ## Terminology\n",
       "        Some common terminology associated with Ollama includes:\n",
       "        - Natural Language Processing (NLP): A subfield of AI that focuses on enabling machines to understand, interpret, and generate human language.\n",
       "        - Speech Recognition: A subfield of NLP that focuses on developing algorithms for transcribing spoken language into text.\n",
       "        - Dialogue Management: A subfield of NLP that focuses on developing algorithms for managing conversations with users, enabling more engaging and personalized interactions.\n",
       "        - Language Modeling: A subfield of NLP that focuses on developing algorithms for predicting the next word or sentence in a sequence, enabling more accurate language generation tasks.\n",
       "        - Machine Learning (ML): A subfield of AI that focuses on developing algorithms for pattern recognition, prediction, and decision-making.\n",
       "        - Big Data: Large volumes of data that require specialized tools and techniques for processing, analysis, and visualization.\n",
       "        - Sentiment Analysis: A subfield of NLP that focuses on analyzing text data and extracting sentiment, enabling businesses to understand customer feedback, monitor social media, and improve brand reputation.\n",
       "        ## Deployments\n",
       "        Example deployments of Ollama include:\n",
       "        - Amazon Lex for building conversational interfaces that can understand and respond to natural language queries: https://aws.amazon.com/lex/\n",
       "        - Google Cloud Natural Language API for analyzing text data and extracting features like sentiment, syntax, and entity recognition: https://cloud.google.com/natural-language/\n",
       "        - Microsoft Azure Cognitive Services for developing language processing applications that can understand, generate, and interact with natural language data: https://azure.microsoft.com/en-us/services/cognitive-services/\n",
       "        ## Resources\n",
       "        To learn more about Ollama, a data scientist can refer to the following resources:\n",
       "        - Tutorials: Websites like Towards Data Science, Analytics Vidhya, and Machine Learning Mastery offer tutorials on Ollama, covering topics like speech recognition, language modeling, and dialogue management.\n",
       "        - Books: Books like \"Natural Language Processing with Python\" by Michael J. C. Cooper provide comprehensive overviews of Ollama, including its history, principles, and applications.\n",
       "        - Authoritative Institutions: Institutions like ACL, EMNLP, and NAACL provide authoritative resources for Ollama, including conferences, workshops, and journals."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment below to see result of one-off run:\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Resource Links\n",
    "\n",
    "This intent with this section is to ask the LLMs to give us a list of resources to learn more on the topic. At this point Google Gemini is giving the best results but typically about 50% are invalid so the current iteration will require heavy editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "rl_examples = [\n",
    "    {   \"question\": \"\"\"Create a list of about 8 to 12 specific resources that a data science \n",
    "        student can look to for developing expertise around Cybersecurity Knowledge Graphs\n",
    "        in order to develop expertise on the most relevant data tools related to this topic.\n",
    "        Please ensure that the references are real and not created by you or inaccurate or \n",
    "        invalid URLs. Please focus on sources that will lead to directly to concrete \n",
    "        knowledge on this topic such as scholarly articles, free online courses, books or \n",
    "        video tutorials.\"\"\",\n",
    "        \"answer\":\"\"\"## Additional Resources:\n",
    "        - MITRE ATT&CK Knowledge Base:** https://attack.mitre.org/\n",
    "        This remains a crucial resource for understanding adversary tactics, techniques, and knowledge (ATT&CK). It provides a standardized framework for classifying cyber threats, essential for populating knowledge graphs.\n",
    "        - Neo4j: Graphs for Cybersecurity:** https://neo4j.com/blog/graphs-cybersecurity-knowledge-graph-digital-twin/\n",
    "        This guide dives into the practical application of knowledge graphs for cybersecurity. Learn how Neo4j, a popular graph database platform, leverages these graphs for threat detection, incident analysis, and attack surface management.\n",
    "        - Papers with Code: Knowledge Graph Embedding:** https://paperswithcode.com/task/knowledge-graph-embedding\n",
    "        Papers with Code is a fantastic resource for staying updated on the latest research in knowledge graph embedding techniques. Knowledge embedding allows for efficient representation and analysis of knowledge graphs within data science models.\n",
    "        - Amazon Web Services (AWS) Knowledge Graphs:** https://aws.amazon.com/neptune/knowledge-graphs-on-aws/\n",
    "        AWS offers Neptune, which is touted as a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Follow the link above for more info on Neptune.\n",
    "        - OpenCypher:** https://opencypher.org/\n",
    "        openCypher is an open source implementation of Cypher - the most widely adopted, fully-specified, and open query language for property graph databases. Cypher was developed by Neo4j. OpenCypher is part of an initiative to create an open standard GQL (Graph Query Language) within the International Organization for Standardization (ISO).\n",
    "        - ResearchGate: Cybersecurity knowledge graphs:** https://www.researchgate.net/publication/370401574_Cybersecurity_knowledge_graphs\n",
    "        This research paper delves into the technical aspects of building cybersecurity knowledge graphs. It explores the use of graph-based data models and discusses the benefits, challenges, and existing research projects in this area.\n",
    "        - **Open-Source Knowledge Graph Frameworks:**\n",
    "        Several open-source knowledge graph frameworks are available, such as Apache Jena and OpenKE. Exploring their documentation and tutorials can provide hands-on experience in building and manipulating knowledge graphs. (You can find more information by searching online for these frameworks)\n",
    "        \"\"\",\n",
    "    },]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "def rl_example_picker(embeddings,n_samples):\n",
    "    rl_example_sel = SemanticSimilarityExampleSelector.from_examples(\n",
    "                                rl_examples,\n",
    "                                embeddings,\n",
    "                                Chroma,\n",
    "                                # Number of examples\n",
    "                                k=n_samples,\n",
    "                                )\n",
    "    return rl_example_sel\n",
    "\n",
    "def rl_question_func(subject):\n",
    "    question = f\"\"\"\n",
    "        Create a list of about 8 to 12 specific resources that a data \n",
    "        science student can look to for developing expertise around \n",
    "        {subject} in order to develop expertise on the most relevant \n",
    "        data tools related to this topic. Ensure that the references \n",
    "        are real and not created by you or inaccurate or invalid URLs.\n",
    "        Focus on sources that will lead to directly to concrete \n",
    "        knowledge on this topic such as scholarly articles, free \n",
    "        online courses, books or video tutorials.\n",
    "        \"\"\"\n",
    "    return question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Create a list of about 8 to 12 specific resources that a data science \n",
      "        student can look to for developing expertise around Cybersecurity Knowledge Graphs\n",
      "        in order to develop expertise on the most relevant data tools related to this topic.\n",
      "        Please ensure that the references are real and not created by you or inaccurate or \n",
      "        invalid URLs. Please focus on sources that will lead to directly to concrete \n",
      "        knowledge on this topic such as scholarly articles, free online courses, books or \n",
      "        video tutorials.\n",
      "AI: ## Additional Resources:\n",
      "        - MITRE ATT&CK Knowledge Base:** https://attack.mitre.org/\n",
      "        This remains a crucial resource for understanding adversary tactics, techniques, and knowledge (ATT&CK). It provides a standardized framework for classifying cyber threats, essential for populating knowledge graphs.\n",
      "        - Neo4j: Graphs for Cybersecurity:** https://neo4j.com/blog/graphs-cybersecurity-knowledge-graph-digital-twin/\n",
      "        This guide dives into the practical application of knowledge graphs for cybersecurity. Learn how Neo4j, a popular graph database platform, leverages these graphs for threat detection, incident analysis, and attack surface management.\n",
      "        - Papers with Code: Knowledge Graph Embedding:** https://paperswithcode.com/task/knowledge-graph-embedding\n",
      "        Papers with Code is a fantastic resource for staying updated on the latest research in knowledge graph embedding techniques. Knowledge embedding allows for efficient representation and analysis of knowledge graphs within data science models.\n",
      "        - Amazon Web Services (AWS) Knowledge Graphs:** https://aws.amazon.com/neptune/knowledge-graphs-on-aws/\n",
      "        AWS offers Neptune, which is touted as a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Follow the link above for more info on Neptune.\n",
      "        - OpenCypher:** https://opencypher.org/\n",
      "        openCypher is an open source implementation of Cypher - the most widely adopted, fully-specified, and open query language for property graph databases. Cypher was developed by Neo4j. OpenCypher is part of an initiative to create an open standard GQL (Graph Query Language) within the International Organization for Standardization (ISO).\n",
      "        - ResearchGate: Cybersecurity knowledge graphs:** https://www.researchgate.net/publication/370401574_Cybersecurity_knowledge_graphs\n",
      "        This research paper delves into the technical aspects of building cybersecurity knowledge graphs. It explores the use of graph-based data models and discusses the benefits, challenges, and existing research projects in this area.\n",
      "        - **Open-Source Knowledge Graph Frameworks:**\n",
      "        Several open-source knowledge graph frameworks are available, such as Apache Jena and OpenKE. Exploring their documentation and tutorials can provide hands-on experience in building and manipulating knowledge graphs. (You can find more information by searching online for these frameworks)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "rl_example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\",\"{question}\"), (\"ai\",\"{answer}\")],\n",
    ")\n",
    "\n",
    "print(rl_example_prompt.format(**rl_examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_model_run(embeddings, n_samples, example_prompt, subject, model):\n",
    "    example_sel=rl_example_picker(embeddings,n_samples)\n",
    "    question = rl_question_func(subject)\n",
    "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "        example_selector=example_sel,\n",
    "        example_prompt=example_prompt,\n",
    "        # suffix=\"Question: {input}\",\n",
    "        input_variables=[\"question\"],\n",
    "        )\n",
    "    \n",
    "    rl_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "        SystemMessage(\n",
    "            content=(\"\"\"Return the response to the question in markdown format with a list of real and\n",
    "                     verifiable resources with a brief description of each resource. When citing references\n",
    "                     only use real references and exclude any generated sources that do not exist.\n",
    "                     \"\"\"\n",
    "            )\n",
    "        ),\n",
    "        few_shot_prompt,\n",
    "        HumanMessage(content=question)\n",
    "        ]\n",
    "        )\n",
    "    chain = rl_prompt | model\n",
    "    output = chain.invoke({\"question\":question})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-off resource link prompt:\n",
    "\n",
    "rl_output = rl_model_run(baa, 1, rl_example_prompt, \"Network Security Technologies\", gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Certainly! Below is a curated list of resources that can help a data science student develop expertise around Network Security Technologies. These resources include scholarly articles, free online courses, books, and video tutorials.\\n\\n### Online Courses\\n\\n1. **Cybrary - Network Security:**\\n   - A comprehensive course covering the fundamentals of network security, including firewalls, VPNs, and intrusion detection systems.\\n   - URL: [https://www.cybrary.it/course/network-security/](https://www.cybrary.it/course/network-security/)\\n\\n2. **Coursera - Introduction to Cyber Security Specialization by NYU:**\\n   - Offers a series of courses that provide a comprehensive overview of network security principles and practices.\\n   - URL: [https://www.coursera.org/specializations/intro-cyber-security](https://www.coursera.org/specializations/intro-cyber-security)\\n\\n### Books\\n\\n3. **\"Network Security Essentials\" by William Stallings:**\\n   - A textbook that covers the key principles of network security and the latest technologies in this field.\\n   - ISBN-13: 978-0134527338\\n\\n4. **\"Practical Packet Analysis\" by Chris Sanders:**\\n   - Focuses on using Wireshark for network security analysis and troubleshooting.\\n   - ISBN-13: 978-1593278021\\n\\n### Scholarly Articles\\n\\n5. **IEEE Xplore Digital Library:**\\n   - A rich source for scholarly articles on network security technologies. Use the search function to find relevant papers.\\n   - URL: [https://ieeexplore.ieee.org/](https://ieeexplore.ieee.org/)\\n\\n6. **Google Scholar:**\\n   - Another excellent resource for finding academic papers on specific network security topics.\\n   - URL: [https://scholar.google.com/](https://scholar.google.com/)\\n\\n### Video Tutorials\\n\\n7. **NetworkChuck on YouTube â€“ Network Security Playlist:**\\n   - Offers practical video tutorials on various network security topics, including VPNs, firewalls, and ethical hacking basics.\\n   - URL: [NetworkChuck Channel](https://www.youtube.com/c/NetworkChuck)\\n\\n8. **Eli the Computer Guy â€“ Networking Security Lessons Playlist:**\\n   - Provides in-depth video lessons covering different aspects of network security from a practical perspective.\\n   - URL: [Eli the Computer Guy Channel](https://www.youtube.com/user/elithecomputerguy)\\n\\n### Tools & Documentation\\n\\n9. **Wireshark Documentation and Tutorials:** \\n    - Learn how to use Wireshark for network analysis and security auditing.\\n    - URL: [https://www.wireshark.org/docs/](https://www.wireshark.org/docs/)\\n\\n10. **Nmap Network Scanning by Gordon â€œFyodorâ€ Lyon:** \\n     - The official guide to Nmap, a powerful network scanning tool used in penetration testing and network troubleshooting.\\n     - URL (Book): [http://nmap.org/book/](http://nmap.org/book/)\\n\\n11. **OWASP Foundation:** \\n     - Provides free resources on web application security best practices, including tools and documentation relevant to securing networks against web-based threats.\\n     - URL: [https://owasp.org/](https://owasp.org/)\\n\\n12. **SANS Institute Reading Room â€“ Network Security Whitepapers:** \\n     - Offers an extensive collection of whitepapers on various aspects of network security written by cybersecurity professionals.\\n     - URL: [https://www.sans.org/reading-room/whitepapers/networks](https://www.sans.org/reading-room/whitepapers/networks)\\n\\nThese resources cover a broad spectrum of knowledge areas within network security technologiesâ€”from foundational concepts to advanced techniquesâ€”and should serve as valuable learning materials for anyone looking to deepen their understanding in this domain.', response_metadata={'token_usage': {'completion_tokens': 788, 'prompt_tokens': 759, 'total_tokens': 1547}, 'model_name': 'gpt-4-0125-preview', 'system_fingerprint': 'fp_b77cb481ed', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('backup', 'output', 'rl_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Setup our Functions and Dataframe for Storing Output\n",
    "\n",
    "Here we will set up a dataframe to store our outputs, and setup our function to iterate through each of our processes. We want to create the knowledge object for each subject and store them in the KO column, then create the list of resources and store in the resource column, then extract key terms and store in the key terms column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the DF to store the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>KO</th>\n",
       "      <th>Resources</th>\n",
       "      <th>Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ollama</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phishing and Social Engineering Detection</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vulnerability Assessment and Management</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word Embeddings</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Splunk</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLM Performance Evaluation</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Behavioral Analytics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Subject    KO Resources Terms\n",
       "0                                     Ollama  None      None  None\n",
       "1  Phishing and Social Engineering Detection  None      None  None\n",
       "2    Vulnerability Assessment and Management  None      None  None\n",
       "3                            Word Embeddings  None      None  None\n",
       "4                                     Splunk  None      None  None\n",
       "5                 LLM Performance Evaluation  None      None  None\n",
       "6                       Behavioral Analytics  None      None  None"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataframe creation\n",
    "import pandas as pd\n",
    "\n",
    "ko_df = topic_list.copy()\n",
    "ko_df['KO'] = None\n",
    "ko_df['Resources'] = None\n",
    "ko_df['Terms'] = None\n",
    "ko_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Function to Make our KOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43081.19 ms\n",
      "llama_print_timings:      sample time =    2003.87 ms / 15237 runs   (    0.13 ms per token,  7603.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 8585315.82 ms / 15237 runs   (  563.45 ms per token,     1.77 tokens per second)\n",
      "llama_print_timings:       total time = 8768157.40 ms / 15238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43081.19 ms\n",
      "llama_print_timings:      sample time =     232.61 ms /  1765 runs   (    0.13 ms per token,  7587.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  184720.30 ms /  2346 tokens (   78.74 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:        eval time =  614214.49 ms /  1764 runs   (  348.19 ms per token,     2.87 tokens per second)\n",
      "llama_print_timings:       total time =  805002.70 ms /  4110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43081.19 ms\n",
      "llama_print_timings:      sample time =     199.66 ms /  1531 runs   (    0.13 ms per token,  7668.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24240.61 ms /   311 tokens (   77.94 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:        eval time =  495795.96 ms /  1530 runs   (  324.05 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:       total time =  524825.59 ms /  1841 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43081.19 ms\n",
      "llama_print_timings:      sample time =     185.79 ms /  1417 runs   (    0.13 ms per token,  7626.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  213244.63 ms /  2648 tokens (   80.53 ms per token,    12.42 tokens per second)\n",
      "llama_print_timings:        eval time =  477488.91 ms /  1416 runs   (  337.21 ms per token,     2.97 tokens per second)\n",
      "llama_print_timings:       total time =  695296.37 ms /  4064 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43081.19 ms\n",
      "llama_print_timings:      sample time =     188.06 ms /  1427 runs   (    0.13 ms per token,  7588.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =  173366.65 ms /  2286 tokens (   75.84 ms per token,    13.19 tokens per second)\n",
      "llama_print_timings:        eval time =  478815.60 ms /  1426 runs   (  335.78 ms per token,     2.98 tokens per second)\n",
      "llama_print_timings:       total time =  656875.34 ms /  3712 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43081.19 ms\n",
      "llama_print_timings:      sample time =     214.78 ms /  1608 runs   (    0.13 ms per token,  7486.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28854.31 ms /   275 tokens (  104.92 ms per token,     9.53 tokens per second)\n",
      "llama_print_timings:        eval time =  589971.84 ms /  1607 runs   (  367.13 ms per token,     2.72 tokens per second)\n",
      "llama_print_timings:       total time =  624488.78 ms /  1882 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   43081.19 ms\n",
      "llama_print_timings:      sample time =    4008.84 ms / 30325 runs   (    0.13 ms per token,  7564.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21221.07 ms /   263 tokens (   80.69 ms per token,    12.39 tokens per second)\n",
      "llama_print_timings:        eval time = 24136440.16 ms / 30324 runs   (  795.95 ms per token,     1.26 tokens per second)\n",
      "llama_print_timings:       total time = 24853973.66 ms / 30587 tokens\n"
     ]
    }
   ],
   "source": [
    "# Function for KO creation\n",
    "import pandas as pd\n",
    "\n",
    "#Define the function to iterate through the df and add kos\n",
    "def apply_ko_model_run(row, embeddings, n_samples, example_prompt, model):\n",
    "    output = \"\"\n",
    "    #Added while loop because some outputs were incorrect and length seemed the best test.\n",
    "    while len(output.split()) < 350:\n",
    "        ai_output = ko_model_run(embeddings, n_samples, example_prompt, row['Subject'], model)\n",
    "        \n",
    "        #try for open_ai output except for others\n",
    "        try:\n",
    "            output = ai_output.content\n",
    "        except:\n",
    "            output = ai_output\n",
    "    \n",
    "    # Return the output to be stored in the 'KO' column\n",
    "\n",
    "    return output\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "ko_df['KO'] = ko_df.apply(apply_ko_model_run, axis=1, args=(baa, 1, example_prompt, herm_cpp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI: Word Embeddings\n",
      "            ## Overview\n",
      "            Word Embeddings are a type of natural language processing technique that maps words or phrases from a vocabulary to vectors of real numbers. These vectors capture semantic relationships between words, making it easier for machines to understand, interpret, and generate human language. Word Embeddings have gained significant traction in recent years, enabling applications like machine translation, sentiment analysis, and text classification.\n",
      "            ## History\n",
      "            The history of Word Embeddings can be traced back to the early days of AI, with researchers exploring techniques for natural language understanding, reasoning, and knowledge representation. However, it was not until the advent of big data and the internet that Word Embeddings gained significant traction, enabling organizations to manage and make sense of vast amounts of information.\n",
      "            ## Applications\n",
      "            Word Embeddings have numerous applications across various domains, including:\n",
      "            - Machine Translation: Word Embeddings can be used to translate text from one language to another, enabling cross-lingual communication and information sharing.\n",
      "            - Sentiment Analysis: Word Embeddings can be used to analyze sentiment in text data, enabling organizations to understand customer feedback, monitor social media, and improve customer experience.\n",
      "            - Text Classification: Word Embeddings can be used to classify text data into categories like news articles, product reviews, and customer feedback, enabling organizations to gain insights into user behavior, preferences, and needs.\n",
      "            - Information Retrieval: Word Embeddings can be used to retrieve relevant information from large-scale document collections, enabling organizations to answer questions, solve problems, and make decisions.\n",
      "            - Chatbots: Word Embeddings can be used to power chatbots, enabling machines to understand and respond to natural language queries, commands, and requests.\n",
      "            ## When to use Word Embeddings\n",
      "            Word Embeddings should be utilized when dealing with text data that requires semantic analysis, reasoning, and inference. It is particularly useful for tasks that involve:\n",
      "            - Identifying relationships between words and concepts.\n",
      "            - Analyzing sentiment, tone, and polarity.\n",
      "            - Classifying text data into categories.\n",
      "            - Retrieving relevant information from large-scale document collections.\n",
      "            - Enhancing chatbot performance with natural language understanding.\n",
      "            ## Technologies\n",
      "            To become an expert in Word Embeddings, a data scientist should focus on the following technologies:\n",
      "            - Word2Vec: A popular Word Embedding algorithm that maps words to vectors of real numbers, capturing semantic relationships between them.\n",
      "            - GloVe: Another Word Embedding algorithm that uses a matrix factorization approach to learn word embeddings, capturing local context and global relationships.\n",
      "            - FastText: A Word Embedding algorithm that learns word embeddings from raw character n-grams, enabling efficient training on large-scale text data.\n",
      "            - ELMo: A Word Embedding algorithm that learns contextualized embeddings by encoding sentence-level semantics, enabling more accurate analysis and prediction tasks.\n",
      "            - TensorFlow: A popular programming library for developing Word Embedding applications, with support for distributed training, parallelization, and model optimization.\n",
      "            ## Strengths and Limitations\n",
      "            Strengths of Word Embeddings include its ability to capture semantic relationships between words, support large-scale text analysis, and enable cross-lingual communication. However, it also has some limitations, such as data quality issues, performance challenges, and the need for expertise in various domains. Additionally, Word Embeddings may not be widely adopted in some organizations due to concerns about data privacy, security, and ownership.\n",
      "            ## Alternative Options\n",
      "            Alternative options to Word Embeddings include manual feature extraction, rule-based systems, and traditional data analysis techniques like regression, clustering, and classification. However, these methods may not scale well with big data or provide the same level of context-awareness as Word Embeddings.\n",
      "            ## Terminology\n",
      "            Some common terminology associated with Word Embeddings includes:\n",
      "            - Word Embedding: A vector representation of a word, capturing its semantic relationships with other words in a vocabulary.\n",
      "            - Semantic Relationship: A connection between two words, representing a type of association or interaction (e.g., \"related to,\" \"similar to,\" \"opposite of\").\n",
      "            - Distributed Training: A technique for training large-scale machine learning models across multiple nodes, enabling faster convergence and better performance.\n",
      "            - Parallelization: A technique for executing multiple tasks simultaneously, enabling faster processing and lower latency.\n",
      "            - Contextualized Embedding: An embedding that captures the meaning of a word based on its context within a sentence or document, enabling more accurate analysis and prediction tasks.\n",
      "            - N-gram: A contiguous sequence of n items from a given text or corpus, often used as input to Word Embedding algorithms.\n",
      "            - Vector Space Model: A mathematical model that represents words as vectors in a high-dimensional space, enabling machines to perform semantic analysis and reasoning.\n",
      "            ## Deployments\n",
      "            Example deployments of Word Embeddings include:\n",
      "            - Google Translate for translating text from one language to another: https://translate.google.com/\n",
      "            - Amazon Comprehend for analyzing sentiment, tone, and topic in text data: https://aws.amazon.com/comprehend/\n",
      "            - IBM Watson for powering chatbots, answering questions, and solving problems: https://www.ibm.com/watson/\n",
      "            ## Resources\n",
      "            To learn more about Word Embeddings, a data scientist can refer to the following resources:\n",
      "            - Tutorials: Websites like KDnuggets, Analytics Vidhya, and Machine Learning Mastery offer tutorials on Word Embeddings, covering topics like algorithm selection, hyperparameter tuning, and application development.\n",
      "            - Books: Books like \"Word Embeddings\" by Arpit Mittal provide comprehensive overviews of Word Embeddings, including their history, principles, and applications.\n",
      "            - Research Papers: Research papers like \"Word2Vec: A Skip-gram Language Model\" by Tom B. Brown, Benjamin Mann, et al., provide detailed explanations of Word Embedding algorithms, their limitations, and their applications.\n"
     ]
    }
   ],
   "source": [
    "#Check the output\n",
    "\n",
    "print(ko_df.loc[3,'KO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('backup2', 'ko_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Resource Lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for resource list creation\n",
    "import pandas as pd\n",
    "\n",
    "#Define the function to iterate through the df and add the resorce texts\n",
    "def apply_resource_model_run(row, embeddings, n_samples, example_prompt, model):\n",
    "    output = rl_model_run(embeddings, n_samples, example_prompt, row['Subject'], model)\n",
    "    # Return the output, openai usually returns as AIMessage type so added the try .content\n",
    "    try:\n",
    "        return output.content\n",
    "    except:\n",
    "        return output\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "ko_df['Resources'] = ko_df.apply(apply_resource_model_run, axis=1, args=(baa, 1, rl_example_prompt, gpt_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Hugging Face has become a pivotal platform in the data science and machine learning community, especially for those working with natural language processing (NLP). Below is a curated list of resources that can help a data science student develop expertise around Hugging Face and its ecosystem:\n",
      "\n",
      "1. **Hugging Face's Official Website:** https://huggingface.co/\n",
      "   - The primary source for all things related to Hugging Face, including documentation, model repositories, and community forums.\n",
      "\n",
      "2. **Transformers Library Documentation:** https://huggingface.co/docs/transformers/index\n",
      "   - Detailed documentation on the Transformers library by Hugging Face, which is crucial for understanding how to implement state-of-the-art NLP models.\n",
      "\n",
      "3. **Hugging Face Course:** https://huggingface.co/course/chapter1\n",
      "   - A free course offered by Hugging Face that covers the fundamentals of Transformers, how to use pre-trained models for various NLP tasks, and how to contribute back to the community.\n",
      "\n",
      "4. **\"Natural Language Processing in Action\" by Lane, Howard, and Hapke:** https://www.manning.com/books/natural-language-processing-in-action\n",
      "   - While not exclusively about Hugging Face, this book provides a solid foundation in NLP upon which one can build using tools like those provided by Hugging Face.\n",
      "\n",
      "5. **YouTube Tutorial: \"Hugging Face Transformers: Fine-tuning and Deployment\":** https://www.youtube.com/watch?v=1pedAIvTWXk\n",
      "   - A practical video tutorial that guides viewers through fine-tuning pre-trained models from Hugging Face and deploying them.\n",
      "\n",
      "6. **DistilBERT Paper (A Model Available on Hugging Face):** https://arxiv.org/abs/1910.01108\n",
      "   - Understanding the research behind some of the models available on Hugging Face can provide deeper insights into their functionality. DistilBERT is one such model that balances performance with efficiency.\n",
      "\n",
      "7. **GitHub Repository for Transformers:** https://github.com/huggingface/transformers\n",
      "   - The GitHub repo contains code, examples, and issues where you can learn from real-world problems and solutions others have encountered.\n",
      "\n",
      "8. **Towards Data Science Article: \"The Illustrated Transformer\":** https://towardsdatascience.com/the-illustrated-transformer-65657e4f123c\n",
      "   - Provides an excellent visual and conceptual understanding of how Transformer models work, which underpin many of the models in the Hugging Face ecosystem.\n",
      "\n",
      "9. **PyTorch Integration Documentation:** https://huggingface.co/docs/transformers/pytorch\n",
      "    - Since many users will be utilizing PyTorch with Transformers, this documentation specifically focuses on integrating the two effectively.\n",
      "\n",
      "10. **TensorFlow Integration Documentation:** https://huggingface.co/docs/transformers/tensorflow\n",
      "    - For TensorFlow users, this part of the documentation explains how to leverage Transformers within TensorFlow projects.\n",
      "\n",
      "11. **Hugging Face Community Forum:** https://discuss.huggingface.co/\n",
      "    - A place to ask questions, share projects, and connect with other practitioners working with Hugging Face technologies.\n",
      "\n",
      "12. **Research Papers Published by Hugging Face:** https://huggingface.co/research/publications\n",
      "    - To stay at the forefront of NLP research and understand the theoretical underpinnings of new models or techniques introduced by Hugging Face.\n",
      "\n",
      "These resources collectively offer a comprehensive pathway to gaining expertise in utilizing Huggling face for various data science applications focused on natural language processing and beyond.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check the output:\n",
    "\n",
    "print(ko_df.loc[3,'Resources'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('backup2', 'ko_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to add Keyword List to KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pete/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Function for keyword extraction with yake:\n",
    "import yake\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Download the stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define a function to apply keyword extraction to the 'KO' column\n",
    "def extract_keywords(row):\n",
    "    \n",
    "    kw_list = []\n",
    "    text = row['KO']\n",
    "    custom_words = row['Subject']\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    english_stopwords.extend(custom_words)\n",
    "    \n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "    n=3,\n",
    "    top=5,\n",
    "    lan='en',\n",
    "    dedupLim=0.1,\n",
    "    dedupFunc='seqm',\n",
    "    stopwords=english_stopwords,\n",
    "    windowsSize=1,\n",
    "    features=None\n",
    "    )\n",
    "    \n",
    "    if text:\n",
    "        keywords = kw_extractor.extract_keywords(text)\n",
    "        kw_list.extend((kw) for kw, v in keywords)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #spacy entity extraction\n",
    "    doc = nlp(text)\n",
    "    proper_nouns = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    all_concepts = proper_nouns + entities\n",
    "    unique_concepts = set(all_concepts)\n",
    "\n",
    "    if unique_concepts:\n",
    "        kw_list.extend(unique_concepts)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return kw_list\n",
    "        \n",
    "    \n",
    "# Apply the function to each row and store the results in the 'Terms' column\n",
    "ko_df['Terms'] = ko_df.apply(extract_keywords, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Brief History', 'Shot Learning', 'FSL models', 'new', 'adapt', 'Tutorials', 'Graph', 'https://arxiv.org/abs/1904.05046\\n', 'Intermediate', 'Recognition', 'Snell et al.', '2021', 'Paperspace', 'Learning', '## Strengths', '## Core Principles', 'Imaging', 'Snell et al', 'Neural', 'Computer Vision', 'PyTorch', 'Vision', 'Language', 'Character', '##', 'Networks', 'https://www.youtube.com/watch?v=efL8S9udCxY', 'Tour', 'Courses', '## Alternative', 'K', 'Yannic', '1', 'Mini', '.', '## Example Deployments\\n1', 'TensorFlow', 'Prototypical', 'Meta', '600', 'Meta-Learning', 'Finn', 'Alternative', 'Frameworks', 'Matching', 'Mini-ImageNet', 'Model-Agnostic Meta-Learning', 'Relation', 'Limitations', 'Meta Learning and Few Shot Learning', '2020', 'Principles', 'Core', 'Meta-Learning Libraries', 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks', 'Prototypical Networks', '## Limitations', 'al', 'CIFAR', 'Omniglot', 'Resources', 'https://blog.paperspace.com/few-shot-learning/', 'ImageNet', '100', 'Beginner', 'https://arxiv.org/abs/1703.05175', 'Deep', 'One', 'Kilcher', 'Adaptation', 'Semi', 'Analytics', 'FS', 'Wang et al', 'Vinyals', 'YouTube', 'Technologies', '2016', '2017', '#', 'et', 'Fast', 'Medical Imaging', '4', 'FSL', 'Coursera', 'Vidhya', 'https://www.analyticsvidhya.com/blog/2021/05/a-gentle-introduction-to-few-shot-learning/', '## Technologies', 'Natural', 'the mid-2010s', '2', 'Natural Language Processing', 'Online', 'Agnostic', 'Papers', 'MAML', '5', 'Yannic Kilcher', 'Terminology', 'https://www.coursera.org/lecture/meta-learning-few-shot-learning/few-shot-learning-Uh5Ow', 'Matching Networks', 'Reptile', 'Graph Neural Networks', 'Unsupervised', 'Shot', 'Deep Learning Frameworks', 'Common', 'Relation Networks', 'Wang', 'Hubert', '3', 'Model', 'Drug', 'Options', 'A Gentle Introduction to Few-Shot Learning', 'Classification', 'Hubert et al', 'Discovery', 'decades', 'Libraries', 'Snell']\n"
     ]
    }
   ],
   "source": [
    "#Check the output:\n",
    "\n",
    "print(ko_df.loc[0,'Terms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export rows to Markdown files\n",
    "\n",
    "With this step, we'll bring together the rough knowledge object article, the list of resources and the list of keywords into a single rough markdown file. Once this is completed the articles will need to be proof read, fact checked and polished to create an accurate knowledge object to help DS students guide their learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>KO</th>\n",
       "      <th>Resources</th>\n",
       "      <th>Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Few Shot Learning</td>\n",
       "      <td># Few Shot Learning\\n\\n## Core Principles and ...</td>\n",
       "      <td>Certainly! Few-Shot Learning is a fascinating ...</td>\n",
       "      <td>[Brief History, Shot Learning, FSL models, new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reinforcement Learning from Human Feedback</td>\n",
       "      <td>Reinforcement Learning from Human Feedback (RL...</td>\n",
       "      <td>Certainly! Reinforcement Learning from Human F...</td>\n",
       "      <td>[Reinforcement Learning, RLHF, systems, adapt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLM Fine-Tuning</td>\n",
       "      <td>LLM Fine-Tuning: Empowering Language Models fo...</td>\n",
       "      <td>Certainly! Fine-tuning Large Language Models (...</td>\n",
       "      <td>[LLM Fine-Tuning, natural language, pre-traine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hugging Face</td>\n",
       "      <td># Hugging Face: A Comprehensive Guide for Data...</td>\n",
       "      <td>Certainly! Hugging Face has become a pivotal p...</td>\n",
       "      <td>[Hugging Face, NLP, Face tools, pre-trained, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intrusion Detection and Prevention</td>\n",
       "      <td># Intrusion Detection and Prevention\\n\\n## Cor...</td>\n",
       "      <td>Certainly! Below is a curated list of resource...</td>\n",
       "      <td>[IDP, intrusion detection systems, data, logs,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Subject  \\\n",
       "0                            Few Shot Learning   \n",
       "1  Reinforcement Learning from Human Feedback    \n",
       "2                              LLM Fine-Tuning   \n",
       "3                                 Hugging Face   \n",
       "4           Intrusion Detection and Prevention   \n",
       "\n",
       "                                                  KO  \\\n",
       "0  # Few Shot Learning\\n\\n## Core Principles and ...   \n",
       "1  Reinforcement Learning from Human Feedback (RL...   \n",
       "2  LLM Fine-Tuning: Empowering Language Models fo...   \n",
       "3  # Hugging Face: A Comprehensive Guide for Data...   \n",
       "4  # Intrusion Detection and Prevention\\n\\n## Cor...   \n",
       "\n",
       "                                           Resources  \\\n",
       "0  Certainly! Few-Shot Learning is a fascinating ...   \n",
       "1  Certainly! Reinforcement Learning from Human F...   \n",
       "2  Certainly! Fine-tuning Large Language Models (...   \n",
       "3  Certainly! Hugging Face has become a pivotal p...   \n",
       "4  Certainly! Below is a curated list of resource...   \n",
       "\n",
       "                                               Terms  \n",
       "0  [Brief History, Shot Learning, FSL models, new...  \n",
       "1  [Reinforcement Learning, RLHF, systems, adapt,...  \n",
       "2  [LLM Fine-Tuning, natural language, pre-traine...  \n",
       "3  [Hugging Face, NLP, Face tools, pre-trained, I...  \n",
       "4  [IDP, intrusion detection systems, data, logs,...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported KO_rough_Few_Shot_Learning.md\n",
      "Exported KO_rough_Reinforcement_Learning_from_Human_Feedback_.md\n",
      "Exported KO_rough_LLM_Fine-Tuning.md\n",
      "Exported KO_rough_Hugging_Face.md\n",
      "Exported KO_rough_Intrusion_Detection_and_Prevention.md\n",
      "Exported KO_rough_Knowledge_Graphs.md\n",
      "Exported KO_rough_ML_Classification_for_NLP.md\n",
      "Exported KO_rough_Named_Entity_Recognition.md\n",
      "Exported KO_rough_Natural_Language_ToolKit.md\n",
      "Exported KO_rough_Neo4J_Database.md\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "# Function to convert HTML to markdown (may not be necessary)\n",
    "def html_to_markdown(html):\n",
    "    return md(html)\n",
    "\n",
    "# Function to create markdown list from a list of strings\n",
    "def list_to_markdown(lst):\n",
    "    return '\\n'.join(f'- {item}' for item in lst)\n",
    "\n",
    "# Iterate over the DataFrame and create markdown files\n",
    "for index, row in ko_df.iterrows():\n",
    "    \n",
    "    combined_text = '# ' + row['Subject'] + '\\n\\n' + row['KO'] + '\\n\\n' + row['Resources'] + '\\n\\n' + '## Key Terms' + '\\n\\n' + list_to_markdown(row['Terms'])\n",
    "    \n",
    "    # Convert any possible HTML content to markdown (optional)\n",
    "    combined_text = html_to_markdown(combined_text)\n",
    "    \n",
    "    #Define file name and convert to markdown file\n",
    "    row_subj = row['Subject']\n",
    "    name_nospace = row_subj.replace(' ', '_')\n",
    "    file_name = f\"KO_rough_{name_nospace}.md\"\n",
    "    \n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(combined_text)\n",
    "\n",
    "    print(f\"Exported {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
